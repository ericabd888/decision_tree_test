#!/usr/bin/env python
# coding: utf-8

# In[4]:


from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import sklearn.metrics
import numpy as np
import math
import pandas as pd
class Node:
    """A decision tree node."""
    def __init__(self, gini, entropy, num_samples, 
            num_samples_per_class, predicted_class):
        self.gini = gini
        self.entropy = entropy
        self.num_samples = num_samples
        self.num_samples_per_class = num_samples_per_class
        self.predicted_class = predicted_class
        self.feature_index = 0
        self.threshold = 0
        self.left = None
        self.right = None

class DecisionTreeClassifier:
    def __init__(self,criterion='gini', max_depth=4):
        self.criterion = criterion
        self.max_depth = max_depth

    def _gini(self,sample_y,n_classes):
        # TODO: calculate the gini index of sample_y
        # sample_y represent the label of node
        gini, temp, giniList = 1, 0, []
        for i in set(sample_y):
            for j in range(sample_y.shape[0]):
                if i == sample_y[j]:
                    temp= temp + 1
            giniList.append(tuple([i, temp]))
            temp = 0
        for i in range(len(giniList)):
            gini = gini - ((giniList[i][1])/len(sample_y))**2
        return gini
         
    def _entropy(self,sample_y,n_classes):
        # TODO: calculate the entropy of sample_y 
        # sample_y represent the label of node
        entropy, temp, entropyList = 0, 0, []
        for i in set(sample_y):
            for j in range(sample_y.shape[0]):
                if i == sample_y[j]:
                    temp= temp + 1
            entropyList.append(tuple([i, temp]))
            temp = 0
        for i in range(len(entropyList)):
            entropy = entropy - (entropyList[i][1]/len(sample_y))*math.log((entropyList[i][1]/len(sample_y)), 2)        
        return entropy

    def _feature_split(self, X, y,n_classes):
        # Returns:
        #  best_idx: Index of the feature for best split, or None if no split is found.
        #  best_thr: Threshold to use for the split, or None if no split is found.
        m = y.size
        if m <= 1:
            return None, None

        # Gini or Entropy of current node.
        if self.criterion == "gini":
            best_criterion = self._gini(y,n_classes)
        else:
            best_criterion = self._entropy(y,n_classes)

        best_idx, best_thr = None, None

        # TODO: find the best split, loop through all the features, and consider all the
        # midpoints between adjacent training samples as possible thresholds. 
        # Computethe Gini or Entropy impurity of the split generated by that particular feature/threshold
        # pair, and return the pair with smallest impurity.
        if self.criterion == 'gini':
            checkSetSize = len(set(y))
            if checkSetSize <= 1:
                return None, None
            combineXY = np.c_[X, y]
            left, right, allGini, tempGini = [], [], [], 0
            for j in range(X.shape[1]):
                for k in range(min(X[:, j]), max(X[:, j])):
                    for i in range(X.shape[0]):
                        if combineXY[i][j] <= k:
                            left.append(list(combineXY[i, :]))
                        else:
                            right.append(list(combineXY[i, :]))
                    left = np.array(left)
                    right =  np.array(right)
                    tempGini = (left.shape[0]/y.shape[0])*self._gini(left[:, -1], n_classes) + \
                               (right.shape[0]/y.shape[0])*self._gini(right[:, -1], n_classes)
                    allGini.append(tuple([j, k, tempGini]))
                    tempGini = 0
                    left, right = [], []
            big = 100
            for i in range(len(allGini)):
                if big > allGini[i][2]:
                    big = allGini[i][2]
                    best_idx, best_thr = allGini[i][0], allGini[i][1]
        else:
            checkSetSize = len(set(y))
            if checkSetSize <= 1:
                return None, None
            combineXY = np.c_[X, y]
            left, right, allGini, tempGini = [], [], [], 0
            for j in range(X.shape[1]):
                for k in range(min(X[:, j]), max(X[:, j])):
                    for i in range(X.shape[0]):
                        if combineXY[i][j] <= k:
                            left.append(list(combineXY[i, :]))
                        else:
                            right.append(list(combineXY[i, :]))
                    left = np.array(left)
                    right =  np.array(right)
                    tempGini = (left.shape[0]/y.shape[0])*self._entropy(left[:, -1], n_classes) + \
                               (right.shape[0]/y.shape[0])*self._entropy(right[:, -1], n_classes)
                    allGini.append(tuple([j, k, tempGini]))
                    tempGini = 0
                    left, right = [], []
            big = 100
            for i in range(len(allGini)):
                if big > allGini[i][2]:
                    big = allGini[i][2]
                    best_idx, best_thr = allGini[i][0], allGini[i][1]           
        return best_idx, best_thr
    def _build_tree(self, X, y, depth=0, traversal_path = []):
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        node = Node(
            gini=self._gini(y,self.n_classes_),
            entropy = self._entropy(y,self.n_classes_),
            num_samples=y.size,
            num_samples_per_class=num_samples_per_class,
            predicted_class=predicted_class,
        )
        if depth < self.max_depth:
            idx, thr = self._feature_split(X, y,self.n_classes_)
            if idx is not None:
            # TODO: Split the tree recursively according index and threshold until maximum depth is reached.
                splitTree = np.c_[X, y]
                left, right = [], []
                for i in range(X.shape[0]):                   
                    if splitTree[i][idx] <= thr:
                        left.append(list(splitTree[i, :]))
                    else:
                        right.append(list(splitTree[i, :]))
                left = np.array(left)
                right = np.array(right)
#                 print('Ysize, depth, idx, thr, class_num, class:',y.shape[0], depth, idx, thr, num_samples_per_class, predicted_class, y)
                traversal_path.append([depth, idx, thr, predicted_class])   
                self._build_tree(left[:, : X.shape[1]], left[:, -1], depth + 1, traversal_path)
                self._build_tree(right[:, : X.shape[1]], right[:, -1], depth + 1, traversal_path)
                pass
            else:
                traversal_path.append([depth, idx, thr, predicted_class])  
#                 print('Ysize, depth, idx, thr, class_num, class:',y.shape[0], depth, idx, thr, num_samples_per_class, predicted_class, y)
        return traversal_path

    def fit(self,X,Y):
        # Fits to the given training data
        self.n_classes_ = len(np.unique(Y)) 
        self.n_features_ = X.shape[1]
        
        # if user entered a value which was neither gini nor entropy
        if self.criterion != 'gini' :
            if self.criterion != 'entropy':
                self.criterion='gini'         
        self.tree_ = self._build_tree(X, Y, depth = 0, traversal_path = [])

    def predict(self,X, tree):
        pred = []
        makesure = np.array(tree)
        maxnum = max(makesure[:, 0])
        num = 1
        tree[0].append(1)
        for i in range(1, makesure.shape[0]):
            if makesure[i - 1, 0] < makesure[i, 0]:
                num = num * 2
                tree[i].append(num)
            elif makesure[i - 1, 0] == makesure[i, 0]:
                num = num + 1
                tree[i].append(num)
            else:
                num = (num + 1)//(2 ** (makesure[i - 1, 0] - makesure[i, 0]))
                tree[i].append(num)
        newList = []
        for i in range(1, 2 ** (maxnum + 1)):
            for j in range(len(tree)):
                if tree[j][4] == i:
                    newList.append(tree[j][:])
        tree_num_class, find_class = [], []
        newList = np.array(newList)
        someList = newList[:, 4]
        for i in range(len(tree)):
            if ((newList[i, 4] * 2) not in list(someList)):
                tree_num_class.append((newList[i, 4], newList[i, 3], i))
                find_class.append(newList[i, 4])
        tree_num = 1
        List = []
        countList = newList[:, 4]
        countList = list(countList)
        for i in range(X.shape[0]):
            for j in range(len(countList)):
                if tree_num not in find_class:
                    if X[i, newList[countList.index(tree_num), 1]] <= newList[countList.index(tree_num), 2]:
                        tree_num = tree_num * 2
                    else:
                        tree_num = tree_num * 2 + 1
                else:
                    List.append(tree_num)
                    break
            tree_num = 1
        for i in range(len(List)):
            for j in range(len(tree_num_class)):
                if List[i] == tree_num_class[j][0]:
                    pred.append(tree_num_class[j][1])
        print(np.array(pred))
        #TODO: predict the label of data
        return pred

def load_train_test_data(test_ratio=.3, random_state = 1):
    balance_scale = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data",
            names=['Class Name', 'Left-Weigh', 'Left-Distance', 'Right-Weigh','Right-Distance'],header=None)
    
    class_le = LabelEncoder()
    balance_scale['Class Name'] = class_le.fit_transform(balance_scale['Class Name'].values)
    X = balance_scale.iloc[:,1:].values
    y = balance_scale['Class Name'].values
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size = test_ratio, random_state=random_state, stratify=y)
    return X_train, X_test, y_train, y_test

# def scale_features(X_train, X_test):
#     sc = StandardScaler()
#     sc.fit(X_train)
#     X_train_std = sc.transform(X_train)
#     X_test_std = sc.transform(X_test)
#     return X_train_std , X_test_std

def accuracy_report(X_train_scale, y_train,X_test_scale,y_test,criterion='gini',max_depth = 4):
    tree = DecisionTreeClassifier(criterion = criterion, max_depth=max_depth)
    tree.fit(X_train_scale, y_train)
    pred = tree.predict(X_train_scale, tree.tree_)
    print(criterion + " tree train accuracy: %f" 
        % (sklearn.metrics.accuracy_score(y_train, pred )))
    pred = tree.predict(X_test_scale, tree.tree_)
    print(criterion + " tree test accuracy: %f" 
        % (sklearn.metrics.accuracy_score(y_test, pred )))
    
def main(): 
    X_train, X_test, y_train, y_test = load_train_test_data(test_ratio=.3,random_state = 1)
#     X_train_scale, X_test_scale = scale_features(X_train, X_test)
    # gini tree
    accuracy_report(X_train, y_train,X_test, y_test,criterion='gini',max_depth=4)
    # entropy tree
    accuracy_report(X_train, y_train,X_test,y_test,criterion='entropy',max_depth=4) 

if __name__ == "__main__":
    main()
    get_ipython().run_line_magic('load', 'decision_tree.py')


# In[ ]:




